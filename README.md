# Equality-Constrained Quadratic Programming Solver

### Aim of the Project

This repository implements and benchmarks a suite of solvers for a specific class of equality-constrained quadratic programs (QP). The goal is to analyze the trade-offs between direct, iterative, and reduced-space methods for solving large-scale optimization problems, with a practical application in portfolio optimization.

---

### Problem Formulation

The project focuses on solving the following QP:

$$
\begin{aligned}
\min_{x\in\mathbb R^n}\quad & \sum_{i=1}^n x_i^2 - \sum_{i=1}^{n-1} x_i\,x_{i+1} + \sum_{i=1}^n x_i,\\
\text{s.t.}\quad
& \sum_{i=k,\,i\equiv k\;(\mathrm{mod}\;K)} x_i \;=\;\epsilon,\quad k=1,\dots,K,
\end{aligned}
$$

[cite_start]where `x` is the vector of decision variables, `Q` is a tridiagonal Hessian matrix, and `A` is a sparse constraint matrix[cite: 1, 8].

---

### Methodologies Implemented

[cite_start]Four distinct solver strategies based on the Karush-Kuhn-Tucker (KKT) conditions were implemented and compared[cite: 18]:

* [cite_start]**Dense Direct KKT**: Constructs and solves the full `(n+K) x (n+K)` KKT system using direct LU factorization[cite: 19, 64].
* [cite_start]**Iterative KKT (GMRES)**: Solves the sparse KKT system iteratively using the Generalized Minimal Residual method, tested with and without a simple diagonal preconditioner[cite: 20, 85].
* [cite_start]**Schur Complement Reduction**: Eliminates the primal variable `x` and solves a smaller `K x K` system for the dual variable `Î»`, using both direct and iterative (CG) approaches[cite: 21, 109].
* [cite_start]**Null-Space Method**: Eliminates equality constraints by projecting the problem onto the null space of the constraint matrix `A` and solving a smaller, unconstrained QP[cite: 22, 171].

---

### Key Findings

* [cite_start]**Accuracy vs. Scalability**: **Dense Direct** and **Null-Space** methods are the most accurate (residuals `< 10â»Â¹âµ`) but are computationally expensive ($O(nÂ³)$) and only feasible for `n â‰¤ 2000`[cite: 186, 192].
* **Iterative Performance**: **GMRES** is memory-efficient and fast for large `n`, but its accuracy degrades as the problem size increases. [cite_start]The non-preconditioned version often outperformed the simple diagonal preconditioner[cite: 97, 186, 187].
* **Best Trade-Off**: **Schur Complement** methods offer the best balance. [cite_start]The sparse, iterative version (`Schur-sp`) is extremely fast for large `n` but less accurate, while the banded version (`Schur-band`) maintains good accuracy at a moderate cost[cite: 132, 189].

---

### Application Example: Portfolio Optimization ðŸ“ˆ

[cite_start]The notebook includes a practical finance application to solve the **long-only minimum-variance portfolio problem**[cite: 195].

* [cite_start]**Process**: Fetches S&P 100 stock data, estimates the covariance matrix `Î£`, and solves the QP: $\min w^\top \Sigma w$ subject to $1^\top w=1, w \ge 0$[cite: 199, 204].
* [cite_start]**Evaluation**: The performance of portfolios generated by the custom KKT solvers is compared against the `cvxopt` solver, showing competitive out-of-sample annual returns[cite: 265].

---

### Repository Structure
